{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "import os\n",
    "os.chdir('./unsupervised-pretraining-deeplabcut')\n",
    "import chatbands\n",
    "from chatbands import extract_labeled_data\n",
    "from chatbands import extract_unlabeled_data\n",
    "from chatbands import predict_chAT\n",
    "from chatbands import make_labeled_tif\n",
    "from modified.trainingsetmanipulation import create_training_dataset as pose_training_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more information on the specifications and used parameters of the DeepLabCut model, look at the deeplabcut_fullguide.pdf in /home/bram/chatbands**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "In the following code, new data is analyzed using the trained model. Predictions are made every **step_size** pixels along the slices (If yo.\n",
    "Set the following variables:\n",
    "**inference_path**: the folder with tif files that need to be analyzed. All tif files in this folder will be analyzed.\n",
    "**config_path**: the path to the config.yaml file of the project in which the trained model is located that you want to use.\n",
    "**dest_path**: the folder in which the results should be placed.\n",
    "\n",
    "In the **dest_path** folder, a folder is created for the results of each tif file. Extra files are created, but these are the predictions for the subimages. You can remove these if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_path = \"/home/bram/chatbands/data/inference\"\n",
    "#dest_path = \"/home/bram/chatbands/results\"\n",
    "step_size = 40 # Change step_size if needed\n",
    "inference_path = \"/media/flabs02/LabPapers/RGCRegeneration/Data/singleRGCs/deeplabcutChAT_todo\"\n",
    "dest_path = \"/media/flabs02/LabPapers/RGCRegeneration/Data/singleRGCs/deeplabcutChAT_ori\"\n",
    "project_path = \"/home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/\"\n",
    "config_path = os.path.join(project_path, \"config.yaml\") # Comment if new projectdest_path = \"/home/bram/chatbands/results\"\n",
    "chatbands = [f.split('_chAT')[0] for f in os.listdir(inference_path) if f.endswith('.tif')]\n",
    "extract_unlabeled_data.extract_video(chatbands, inference_path, config_path, step_size=step_size) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, choose the step_size. This step_size should always correspond to the step_size in the previous step. Using the **create_video** and **create_tif** options, you can choose to generate a video or tif file for which the predictions of the model are mapped onto the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/bram/.conda/envs/mousetracker/lib/python3.6/site-packages/deeplabcut/pose_estimation_tensorflow/config.py:43: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-680000 for model /home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/dlc-models/iteration-0/chATbandsSep14-trainset80shuffle1\n",
      "shape:  Tensor(\"pose/Shape:0\", shape=(4,), dtype=int32)\n",
      "shape:  Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "INFO:tensorflow:Restoring parameters from /home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/dlc-models/iteration-0/chATbandsSep14-trainset80shuffle1/train/snapshot-680000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/dlc-models/iteration-0/chATbandsSep14-trainset80shuffle1/train/snapshot-680000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing all the videos in the directory\n",
      "Starting to analyze %  video01172_1L_C20.mp4\n",
      "Video already analyzed! /media/flabs02/LabPapers/RGCRegeneration/Data/singleRGCs/deeplabcutChAT_ori/video01172_1L_C20DeepCut_resnet50_chATbandsSep14shuffle1_680000.h5\n",
      "Starting to analyze %  video01172_1L_C19.mp4\n",
      "Video already analyzed! /media/flabs02/LabPapers/RGCRegeneration/Data/singleRGCs/deeplabcutChAT_ori/video01172_1L_C19DeepCut_resnet50_chATbandsSep14shuffle1_680000.h5\n",
      "Starting to analyze %  video01172_1L_C26.mp4\n",
      "Video already analyzed! /media/flabs02/LabPapers/RGCRegeneration/Data/singleRGCs/deeplabcutChAT_ori/video01172_1L_C26DeepCut_resnet50_chATbandsSep14shuffle1_680000.h5\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n",
      "(328, 719, 717)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(329, 718, 717)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(389, 720, 718)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "chatbands = [(f.split('video')[1]).split('.mp4')[0] for f in os.listdir(os.path.join(project_path, 'subimages')) if f.endswith('.mp4')]\n",
    "predict_chAT.analyze_videos(config_path, gputouse=1, videos=None, videotype='mp4', save_as_csv=True, destfolder=dest_path)\n",
    "for chat in chatbands:\n",
    "    predict_chAT.get_slice_results(os.path.join(inference_path, \"{}_chAT_STD.tif\".format(chat)), os.path.join(dest_path, [f for f in os.listdir(dest_path) if f.endswith('.h5') and chat in f][0]), \n",
    "                                       dest_path, step_size=step_size, create_video=False, create_tif=True) # Change step_size if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
