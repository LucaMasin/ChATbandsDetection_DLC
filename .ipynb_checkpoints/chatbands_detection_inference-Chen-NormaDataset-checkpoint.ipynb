{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "os.chdir('./unsupervised-pretraining-deeplabcut')\n",
    "import chatbands\n",
    "from chatbands import extract_labeled_data\n",
    "from chatbands import extract_unlabeled_data\n",
    "from chatbands import predict_chAT\n",
    "from chatbands import make_labeled_tif\n",
    "from modified.trainingsetmanipulation import create_training_dataset as pose_training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more information on the specifications and used parameters of the DeepLabCut model, look at the deeplabcut_fullguide.pdf in /home/bram/chatbands**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "In the following code, new data is analyzed using the trained model. Predictions are made every **step_size** pixels along the slices (If yo.\n",
    "Set the following variables:\n",
    "**inference_path**: the folder with tif files that need to be analyzed. All tif files in this folder will be analyzed.\n",
    "**config_path**: the path to the config.yaml file of the project in which the trained model is located that you want to use.\n",
    "**dest_path**: the folder in which the results should be placed.\n",
    "\n",
    "In the **dest_path** folder, a folder is created for the results of each tif file. Extra files are created, but these are the predictions for the subimages. You can remove these if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01327_3L_C01_01_chAT_STD.tif\n",
      "(204, 369, 180)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01474_1R_C03_02_chAT_STD.tif\n",
      "(224, 437, 696)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01474_1R_C01_01_chAT_STD.tif\n",
      "(224, 373, 421)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01474_1R_C11_02_chAT_STD.tif\n",
      "(184, 292, 260)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01474_1R_C03_01_chAT_STD.tif\n",
      "(224, 390, 186)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01327_3L_C14_01_chAT_STD.tif\n",
      "(205, 571, 638)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01474_1R_C09_01_chAT_STD.tif\n",
      "(182, 433, 485)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01327_3L_C10_04_chAT_STD.tif\n",
      "(205, 520, 493)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01327_3L_C10_01_chAT_STD.tif\n",
      "(205, 292, 388)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01327_3L_C10_03_chAT_STD.tif\n",
      "(205, 608, 462)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01474_1R_C11_01_chAT_STD.tif\n",
      "(184, 296, 494)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01327_3L_C07_01_chAT_STD.tif\n",
      "(203, 448, 469)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01327_3L_C10_02_chAT_STD.tif\n",
      "(205, 541, 420)\n",
      "Normal axis\n",
      "Modified axis\n",
      "/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary/01474_1R_C03_03_chAT_STD.tif\n",
      "(224, 555, 759)\n",
      "Normal axis\n",
      "Modified axis\n"
     ]
    }
   ],
   "source": [
    "#inference_path = \"/home/bram/chatbands/data/inference\"\n",
    "#dest_path = \"/home/bram/chatbands/results\"\n",
    "step_size = 50 # Change step_size if needed, originally 40\n",
    "side_width = 120 # Originally 100\n",
    "\n",
    "# inference_path = \"/media/flabs02/Data/Norma/new_ChATbands_for_DLCtracing/01114\"# \"/media/flabs02/Users/Chen/deeplabcutChAT_todo\"\n",
    "# inference_path = \"/media/flabs02/Data/Norma/new_ChATbands_for_DLCtracing/oscillations\"# \"/media/flabs02/Users/Chen/deeplabcutChAT_todo\"\n",
    "inference_path = \"/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary\"\n",
    "\n",
    "# dest_path = \"/media/flabs02/Data/Norma/new_ChATbands_for_DLCtracing/oscillations\"#\"/media/flabs02/Users/Chen/deeplabcutChAT_ori\"\n",
    "dest_path = \"/media/areca/LabPapers/WFComputation/Data/Ariadne/temporary\"\n",
    "project_path = \"/home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/\"\n",
    "config_path = os.path.join(project_path, \"config.yaml\") \n",
    "chatbands = [f.split('_chAT')[0] for f in os.listdir(inference_path) if f.endswith('.tif')]#\n",
    "extract_unlabeled_data.extract_video(chatbands, inference_path, config_path, step_size=step_size, side_width=side_width) # Change step_size if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, choose the step_size. This step_size should always correspond to the step_size in the previous step. Using the **create_video** and **create_tif** options, you can choose to generate a video or tif file for which the predictions of the model are mapped onto the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-680000 for model /home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/dlc-models/iteration-0/chATbandsSep14-trainset80shuffle1\n",
      "shape:  Tensor(\"pose/Shape:0\", shape=(4,), dtype=int32)\n",
      "shape:  Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "INFO:tensorflow:Restoring parameters from /home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/dlc-models/iteration-0/chATbandsSep14-trainset80shuffle1/train/snapshot-680000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2020-09-14/dlc-models/iteration-0/chATbandsSep14-trainset80shuffle1/train/snapshot-680000\n",
      "  0%|          | 0/3357 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing all the videos in the directory\n",
      "Starting to analyze %  video01474_1R_C01_01.mp4\n",
      "Loading  video01474_1R_C01_01.mp4\n",
      "Duration of video [s]:  111.9 , recorded with  30.0 fps!\n",
      "Overall # of frames:  3357  found with (before cropping) frame dimensions:  240 224\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3366it [00:22, 148.01it/s]                          \n",
      "  0%|          | 0/6118 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  3357\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01474_1R_C03_02.mp4\n",
      "Loading  video01474_1R_C03_02.mp4\n",
      "Duration of video [s]:  203.93 , recorded with  30.0 fps!\n",
      "Overall # of frames:  6118  found with (before cropping) frame dimensions:  240 224\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6161it [00:38, 158.93it/s]                          \n",
      "  0%|          | 0/1170 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  6118\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01474_1R_C03_01.mp4\n",
      "Loading  video01474_1R_C03_01.mp4\n",
      "Duration of video [s]:  39.0 , recorded with  30.0 fps!\n",
      "Overall # of frames:  1170  found with (before cropping) frame dimensions:  240 224\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1177it [00:07, 156.19it/s]                          \n",
      "  0%|          | 0/5200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  1170\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01327_3L_C10_04.mp4\n",
      "Loading  video01327_3L_C10_04.mp4\n",
      "Duration of video [s]:  173.33 , recorded with  30.0 fps!\n",
      "Overall # of frames:  5200  found with (before cropping) frame dimensions:  240 204\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5252it [00:31, 164.43it/s]                          \n",
      "  0%|          | 0/6080 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  5200\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01327_3L_C10_03.mp4\n",
      "Loading  video01327_3L_C10_03.mp4\n",
      "Duration of video [s]:  202.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  6080  found with (before cropping) frame dimensions:  240 204\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6120it [00:37, 163.83it/s]                          \n",
      "  0%|          | 0/2960 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  6080\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01474_1R_C11_01.mp4\n",
      "Loading  video01474_1R_C11_01.mp4\n",
      "Duration of video [s]:  98.67 , recorded with  30.0 fps!\n",
      "Overall # of frames:  2960  found with (before cropping) frame dimensions:  240 184\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2987it [00:17, 175.09it/s]                          \n",
      "  0%|          | 0/7423 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  2960\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01327_3L_C14_01.mp4\n",
      "Loading  video01327_3L_C14_01.mp4\n",
      "Duration of video [s]:  247.43 , recorded with  30.0 fps!\n",
      "Overall # of frames:  7423  found with (before cropping) frame dimensions:  240 204\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7474it [00:45, 164.23it/s]                          \n",
      "  0%|          | 0/4480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  7423\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01327_3L_C07_01.mp4\n",
      "Loading  video01327_3L_C07_01.mp4\n",
      "Duration of video [s]:  149.33 , recorded with  30.0 fps!\n",
      "Overall # of frames:  4480  found with (before cropping) frame dimensions:  240 202\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4488it [00:27, 162.39it/s]                          \n",
      "  0%|          | 0/1752 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  4480\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01474_1R_C11_02.mp4\n",
      "Loading  video01474_1R_C11_02.mp4\n",
      "Duration of video [s]:  58.4 , recorded with  30.0 fps!\n",
      "Overall # of frames:  1752  found with (before cropping) frame dimensions:  240 184\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1768it [00:10, 174.73it/s]                          \n",
      "  0%|          | 0/4330 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  1752\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01474_1R_C09_01.mp4\n",
      "Loading  video01474_1R_C09_01.mp4\n",
      "Duration of video [s]:  144.33 , recorded with  30.0 fps!\n",
      "Overall # of frames:  4330  found with (before cropping) frame dimensions:  240 182\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4343it [00:22, 189.29it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  4330\n",
      "Saving results in ....\n",
      "Saving csv poses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4869 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  video01327_3L_C10_02.mp4\n",
      "Loading  video01327_3L_C10_02.mp4\n",
      "Duration of video [s]:  162.3 , recorded with  30.0 fps!\n",
      "Overall # of frames:  4869  found with (before cropping) frame dimensions:  240 204\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4896it [00:30, 162.88it/s]                          \n",
      "  0%|          | 0/1107 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  4869\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01327_3L_C01_01.mp4\n",
      "Loading  video01327_3L_C01_01.mp4\n",
      "Duration of video [s]:  36.9 , recorded with  30.0 fps!\n",
      "Overall # of frames:  1107  found with (before cropping) frame dimensions:  240 204\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1111it [00:06, 159.52it/s]                          \n",
      "  0%|          | 0/8880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  1107\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01474_1R_C03_03.mp4\n",
      "Loading  video01474_1R_C03_03.mp4\n",
      "Duration of video [s]:  296.0 , recorded with  30.0 fps!\n",
      "Overall # of frames:  8880  found with (before cropping) frame dimensions:  240 224\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8888it [00:56, 157.07it/s]                          \n",
      "  0%|          | 0/2336 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  8880\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "Starting to analyze %  video01327_3L_C10_01.mp4\n",
      "Loading  video01327_3L_C10_01.mp4\n",
      "Duration of video [s]:  77.87 , recorded with  30.0 fps!\n",
      "Overall # of frames:  2336  found with (before cropping) frame dimensions:  240 204\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2346it [00:14, 161.86it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  2336\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n",
      "(203, 448, 469)\n",
      "Normal axis\n",
      "Modified axis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/bram/.conda/envs/mousetracker/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'decode_packbits'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n",
      "WARNING:py.warnings:/home/bram/.conda/envs/mousetracker/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'decode_lzw'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n",
      "WARNING:py.warnings:/home/bram/.conda/envs/mousetracker/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'unpack_ints'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n",
      "WARNING:py.warnings:/home/bram/.conda/envs/mousetracker/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'reverse_bitorder'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.uint8'>\n",
      "(205, 520, 493)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(182, 433, 485)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(224, 555, 759)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(205, 292, 388)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(184, 296, 494)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(205, 608, 462)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(224, 437, 696)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(184, 292, 260)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(205, 541, 420)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(224, 373, 421)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(224, 390, 186)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(204, 369, 180)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n",
      "(205, 571, 638)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "chatbands = [(f.split('video')[1]).split('.mp4')[0] for f in os.listdir(os.path.join(project_path, 'subimages')) if f.endswith('.mp4')]\n",
    "\n",
    "predict_chAT.analyze_videos(config_path, gputouse=1, videos=None, videotype='mp4', save_as_csv=True, destfolder=dest_path)\n",
    "for chat in chatbands:\n",
    "    predict_chAT.get_slice_results(os.path.join(inference_path, \"{}_chAT_STD.tif\".format(chat)), os.path.join(dest_path, [f for f in os.listdir(dest_path) if f.endswith('.h5') and chat in f][0]), \n",
    "                                       dest_path, step_size=step_size, create_video=False, create_tif=True) # Change step_size if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do you want to rewrite csv files of the ON and OFF ChAT bands?\n",
    "saveToCSV = True\n",
    "savePathCSV = '/media/areca/LabPapers/WFComputation/Data/singleRGCs'\n",
    "reliableThres = .48\n",
    "\n",
    "# get cell folders\n",
    "folderContent = os.listdir(dest_path)\n",
    "cellFolders = []\n",
    "for name in folderContent:\n",
    "    if os.path.isdir(os.path.join(dest_path, name)) and ('STD' in name):\n",
    "        cellFolders.append(name)\n",
    "cellFolders.sort()\n",
    "\n",
    "# prepare plot\n",
    "nCells = len(cellFolders)\n",
    "nRows = int(np.sqrt(nCells))\n",
    "nColumns = int(np.ceil(nCells/nRows))\n",
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "# plot ChAT bands, color represents slice, each subplot is a cell\n",
    "for cellID, folderName in enumerate(cellFolders):\n",
    "    csvPath = os.path.join(dest_path, folderName, 'result_' + folderName + '.csv')\n",
    "    ChATbandCoords = pd.read_csv(csvPath)\n",
    "    nSlices = ChATbandCoords['Slice'].values[-1]+1\n",
    "\n",
    "    plt.subplot(nRows, nColumns, cellID+1)\n",
    "    for retinaSlice in range(nSlices):\n",
    "        takeRows = np.arange(len(ChATbandCoords))[(ChATbandCoords['Slice']==retinaSlice)]\n",
    "        x = ChATbandCoords['X'][takeRows].values\n",
    "        yON = ChATbandCoords['Y_ON'][takeRows].values\n",
    "        yOFF = ChATbandCoords['Y_OFF'][takeRows].values\n",
    "        plt.plot(x, yON, color=plt.cm.viridis(retinaSlice/nSlices), alpha=.5)\n",
    "        plt.plot(x, yOFF, color=plt.cm.viridis(retinaSlice/nSlices), alpha=.5)\n",
    "    plt.title(folderName)\n",
    "    plt.ylim([0, 300])\n",
    "    \n",
    "    ### write new csv files if saveToCSV == True ###\n",
    "    if saveToCSV:\n",
    "        fileNameON = folderName[:-8] + 'ON.txt'\n",
    "        fileNameOFF = folderName[:-8] + 'OFF.txt'\n",
    "        ChATbandCoords['Slice'] = ChATbandCoords['Slice']+1\n",
    "        coordsON = pd.DataFrame(columns=['X', 'Y', 'Slice'])\n",
    "        coordsOFF = pd.DataFrame(columns=['X', 'Y', 'Slice'])\n",
    "        coordsON[['X', 'Y', 'Slice']] = ChATbandCoords[['X', 'Y_ON', 'Slice']]\n",
    "        coordsOFF[['X', 'Y', 'Slice']] = ChATbandCoords[['X', 'Y_OFF', 'Slice']]\n",
    "        \n",
    "        # drop indexes with unreliable values\n",
    "        coordsON = coordsON.drop(coordsON.index[ChATbandCoords['P_ON']<reliableThres])\n",
    "        coordsOFF = coordsOFF.drop(coordsOFF.index[ChATbandCoords['P_OFF']<reliableThres])\n",
    "        \n",
    "        print('Saving csv files %s and %s to %s.' %(fileNameON, fileNameOFF, savePathCSV))\n",
    "        # increment index values to fit matlab read\n",
    "        coordsON.index = range(1,len(coordsON)+1)\n",
    "        coordsOFF.index = range(1,len(coordsOFF)+1)\n",
    "        coordsON[['X', 'Y', 'Slice']].to_csv(os.path.join(savePathCSV, fileNameON), header=False, index=True, sep=',')\n",
    "        coordsOFF[['X', 'Y', 'Slice']].to_csv(os.path.join(savePathCSV, fileNameOFF), header=False, index=True, sep=',')\n",
    "\n",
    "# save figure to ChAT band directory\n",
    "figname = os.path.join(dest_path, 'extractedChATbands_%uCells_reliableThres%.2f' %(nCells, reliableThres))\n",
    "plt.savefig(figname + '.png', dpi = 150, bbox_inches = 'tight')\n",
    "plt.savefig(figname + '.pdf', dpi = 150, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
