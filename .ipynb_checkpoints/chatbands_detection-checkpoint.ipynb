{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mousetracker2/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import deeplabcut\n",
    "import os\n",
    "os.chdir('./unsupervised-pretraining-deeplabcut')\n",
    "import chatbands\n",
    "from chatbands import extract_labeled_data\n",
    "from chatbands import extract_unlabeled_data\n",
    "from chatbands import predict_chAT\n",
    "from chatbands import make_labeled_tif\n",
    "from modified.trainingsetmanipulation import create_training_dataset as pose_training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If no model has been trained, go through the cells in the Training section. If a model has already been trained, you can skip to the Inference section. For more information on the specifications and used parameters of the DeepLabCut model, look at the deeplabcut_fullguide.pdf in /home/bram/chatbands**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "### Load data\n",
    "Set training_path to be the folder containing the training data. The training data consists of tif files with, for each tif file, two excel files which contain the label locations.\n",
    "This step creates a new project in the /home/bram/deeplabcut_test/unsupervised-pretraining-deeplabcut folder containing a config.yaml file. You can modify the settings in this file, but they already have a workeable default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"/home/bram/chatbands/data/training\" # Change if needed\n",
    "chatbands = [f.split('_chAT')[0] for f in os.listdir(training_path) if f.endswith('.tif')]\n",
    "config_path = deeplabcut.create_new_project('chATbands','NERF','',copy_videos=False, videotype='.avi')\n",
    "extract_labeled_data.extract_and_label(chatbands, training_path, config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "The previously created config.yaml file is used to split the given training data up in a training set, validation set and test set. It also creates a pose_cfg.yaml file in the dlc-models folder in the project. This file can be modified but already has workeable default settings. The early_stopping_threshold setting in the pose_cfg.yaml file indicates the number of model saves (save_iters setting) the model does before stopping training due to the performance on the validation set not improving (for more information, look up 'early stopping')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_training_set(config_path, num_shuffles=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "In this step, the DeepLabCut model is actually trained. **gputouse** indicates the GPU used for training and **maxiters** indicates the maximum number of training iterations. This maximum number might not be reached due to the usage of early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modified import training\n",
    "from modified import evaluate\n",
    "\n",
    "training.train_network(config_path, max_snapshots_to_keep=10, gputouse=0, maxiters=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "This step is optional and evaluates the performance of the trained model using its test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.evaluate_network(config_path, plotting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "In the following code, new data is analyzed using the trained model. Predictions are made every **step_size** pixels along the slices (If yo.\n",
    "Set the following variables:\n",
    "**inference_path**: the folder with tif files that need to be analyzed. All tif files in this folder will be analyzed.\n",
    "**config_path**: the path to the config.yaml file of the project in which the trained model is located that you want to use.\n",
    "**dest_path**: the folder in which the results should be placed.\n",
    "\n",
    "In the **dest_path** folder, a folder is created for the results of each tif file. Extra files are created, but these are the predictions for the subimages. You can remove these if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 772, 772)\n",
      "Normal axis\n",
      "Modified axis\n",
      "(256, 772, 772)\n",
      "Normal axis\n",
      "Modified axis\n"
     ]
    }
   ],
   "source": [
    "inference_path = \"/home/bram/chatbands/data/inference\"\n",
    "config_path = \"/home/bram/chatbands/unsupervised-pretraining-deeplabcut/chATbands-NERF-2019-07-02/config.yaml\"\n",
    "dest_path = \"/home/bram/chatbands/results\"\n",
    "chatbands = [f.split('_chAT')[0] for f in os.listdir(inference_path) if f.endswith('.tif')]\n",
    "extract_unlabeled_data.extract_video(chatbands, inference_path, config_path, step_size=20) # Change step_size if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, choose the step_size. This step_size should always correspond to the step_size in the previous step. Using the **create_video** and **create_tif** options, you can choose to generate a video or tif file for which the predictions of the model are mapped onto the slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using snapshot-1000 for model /home/bram/deeplabcut_test/unsupervised-pretraining-deeplabcut/chATbands-NERF-2019-07-02/dlc-models/iteration-0/chATbandsJul2-trainset80shuffle1\n",
      "shape:  Tensor(\"pose/Shape:0\", shape=(4,), dtype=int32)\n",
      "shape:  Tensor(\"Shape:0\", shape=(4,), dtype=int32)\n",
      "INFO:tensorflow:Restoring parameters from /home/bram/deeplabcut_test/unsupervised-pretraining-deeplabcut/chATbands-NERF-2019-07-02/dlc-models/iteration-0/chATbandsJul2-trainset80shuffle1/train/snapshot-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/bram/deeplabcut_test/unsupervised-pretraining-deeplabcut/chATbands-NERF-2019-07-02/dlc-models/iteration-0/chATbandsJul2-trainset80shuffle1/train/snapshot-1000\n",
      "  0%|          | 0/30108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing all the videos in the directory\n",
      "Starting to analyze %  video00628_1L_C06.mp4\n",
      "Loading  video00628_1L_C06.mp4\n",
      "Duration of video [s]:  1003.6 , recorded with  30.0 fps!\n",
      "Overall # of frames:  30108  found with (before cropping) frame dimensions:  200 256\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30401it [02:52, 175.94it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  30108\n",
      "Saving results in ....\n",
      "Saving csv poses!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to analyze %  video00629_2R_C01.mp4\n",
      "Loading  video00629_2R_C01.mp4\n",
      "Duration of video [s]:  1003.6 , recorded with  30.0 fps!\n",
      "Overall # of frames:  30108  found with (before cropping) frame dimensions:  200 162\n",
      "Starting to extract posture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30401it [02:14, 223.07it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected frames:  30108\n",
      "Saving results in ....\n",
      "Saving csv poses!\n",
      "The videos are analyzed. Now your research can truly start! \n",
      " You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!\n",
      "(162, 772, 772)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/opt/conda/envs/mousetracker2/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'decode_packbits'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n",
      "WARNING:py.warnings:/opt/conda/envs/mousetracker2/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'decode_lzw'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n",
      "WARNING:py.warnings:/opt/conda/envs/mousetracker2/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'unpack_ints'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n",
      "WARNING:py.warnings:/opt/conda/envs/mousetracker2/lib/python3.6/site-packages/imageio/plugins/_tifffile.py:7285: UserWarning: module 'imageio.plugins._tifffile' has no attribute 'reverse_bitorder'\n",
      "  Functionality might be degraded or be slow.\n",
      "\n",
      "  warnings.warn(\"%s%s\" % (e, warn))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 772, 772)\n",
      "Normal axis\n",
      "Modified axis\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "predict_chAT.analyze_videos(config_path, videos=None, videotype='mp4', save_as_csv=False, destfolder=dest_path)\n",
    "for chat in chatbands:\n",
    "    predict_chAT.get_slice_results(os.path.join(inference_path, \"{}_chAT_STD.tif\".format(chat)), os.path.join(dest_path, [f for f in os.listdir(dest_path) if f.endswith('.h5') and chat in f][0]), \n",
    "                                       dest_path, step_size=20, create_video=True, create_tif=True) # Change step_size if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
